# -*- coding: utf-8 -*-
"""talentsourcingjobs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/MaadElGali/31cbc319e2419e3c43ba17bb8447ec3c/talentsourcingjobs.ipynb

# Project 3 Potential Talents
"""

# Libraries
import pandas as pd
import numpy as np
from gensim.models.doc2vec import Doc2Vec, TaggedDocument  #word & document embeddings
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from sklearn.model_selection import train_test_split
import xgboost as xgb

!pip install gensim

#Loading a dataset into Google Colab from the local machine
from google.colab import files
uploaded = files.upload()

#Load the dataset

df =pd.read_csv('potential-talents - Aspiring human resources - seeking human resources.csv')

df

#Check for missing data
df.isnull().sum()

#Find all unique values in the 'connection' column
unique_values = df['connection'].unique()
unique_values

#Count the number of occurrences of '500+ ' in the 'connection' column
count_500_plus = df['connection'].value_counts().get('500+ ', 0)
count_500_plus

#Display all rows where the 'connection' column has '500+'
rows_with_500_plus = df[df['connection'] == '500+ ']
rows_with_500_plus

#Step 1
#Keep only the columns 'job_title' and 'connection'
df_small = df[['job_title', 'connection']].copy()

df_small

#Convert all instances of '500+ ' to the numeric value '500' and ensure numeric
df_small['connection'] = df_small['connection'].apply(lambda x: 500 if x == '500+ ' else int(x))

df_small

#Calculate occurrences of each unique value in the 'connection' column
occurrences = df_small['connection'].value_counts()
occurrences

#Step2
#Doc2Vec embeddings for "Aspiring human resources" (w1) and each job_title
#Prepare doc2vec training corpus from job_title strings
documents = [TaggedDocument(words=jt.lower().split(), tags=[str(i)]) for i, jt in enumerate(df_small['job_title'].astype(str))]
d2v_model = Doc2Vec(vector_size=100, min_count=1, epochs=40, workers=4, seed=42)
d2v_model.build_vocab(documents)
d2v_model.train(documents, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)

#Get the embeddings for each job_title
def doc2vec_embed(text):
    return d2v_model.infer_vector(str(text).lower().split())

job_embeds_d2v = np.vstack([doc2vec_embed(t) for t in df_small['job_title']])

#Embedding for query "Aspiring human resources"
w1_d2v = doc2vec_embed("Aspiring human resources")

w1_d2v

#Step 3
#Cosine similarity between w1 and each job_title (doc2vec)
cos_sim_d2v = cosine_similarity(job_embeds_d2v, w1_d2v.reshape(1, -1)).reshape(-1)

cos_sim_d2v

#Step 4
#Build dataframe with job_title, connection, cosine_similarity
df_d2v = df_small[['job_title', 'connection']].copy()
df_d2v['cosine_similarity'] = cos_sim_d2v
df_d2v['connection'] = df_small['connection']

df_d2v

#Step 5
#Scale the connection column to 0-1 (using MinMaxScaler)
scaler = MinMaxScaler()
df_d2v['scaled_connection'] = scaler.fit_transform(df_d2v[['connection']])

df_d2v



#Step 6
#Create the ranking based on the following = 0.8*cosine_similarity + 0.2*scaled_connection
df_d2v['ranking'] = 0.8 * df_d2v['cosine_similarity'] + 0.2 * df_d2v['scaled_connection']

#Step 7
#Sort the df descending by ranking
df_d2v_sorted = df_d2v.sort_values('ranking', ascending=False).reset_index(drop=True)

df_d2v_sorted



#df_d2v_sorted now holds the results for Doc2Vec-only approach
#Get the top-n candidates:
def top_n_d2v(n=10):
    return df_d2v_sorted.head(n)

#Load the sentence-BERT model
sbert_model = SentenceTransformer('all-MiniLM-L6-v2')

#Create the sentence-BERT embeddings for query and job_title
job_titles = df_small['job_title'].astype(str).tolist()
job_embeds_sbert = sbert_model.encode(job_titles, convert_to_numpy=True, normalize_embeddings=False)
w1_sbert = sbert_model.encode(["Aspiring human resources"], convert_to_numpy=True)[0]

#Calculate the cosine similarity between SBERT query and each job_title
cos_sim_sbert = cosine_similarity(job_embeds_sbert, w1_sbert.reshape(1, -1)).reshape(-1)

#Combine Doc2Vec and SBERT
#Combine the similarity scores from both embeddings
#Normalize both similarity arrays to 0-1 before combining
def minmax_arr(a):
    a = np.array(a, dtype=float)
    if a.max() == a.min():
        return np.zeros_like(a)
    return (a - a.min()) / (a.max() - a.min())

sim_d2v_norm = minmax_arr(cos_sim_d2v)
sim_sbert_norm = minmax_arr(cos_sim_sbert)

#Weighted combination of similarities
#Here I give 0.6 to SBERT and 0.4 to Doc2Vec
combined_sim = 0.6 * sim_sbert_norm + 0.4 * sim_d2v_norm

df_combined = df_small[['job_title', 'connection']].copy()
df_combined['cosine_similarity_doc2vec'] = cos_sim_d2v
df_combined['cosine_similarity_sbert'] = cos_sim_sbert
df_combined['combined_similarity'] = combined_sim
df_combined['connection'] = df_small['connection']
df_combined['scaled_connection'] = scaler.fit_transform(df_combined[['connection']])

#Calculate the final ranking using the combined similarity and scaled_connection (0.8 weight to similarity)
df_combined['ranking'] = 0.8 * df_combined['combined_similarity'] + 0.2 * df_combined['scaled_connection']
df_combined_sorted = df_combined.sort_values('ranking', ascending=False).reset_index(drop=True)

#Retrieve Top-n for combined:
def top_n_combined(n=10):
    return df_combined_sorted.head(n)

# End results:
# - df_d2v_sorted: Doc2Vec-only ranking
# - df_sbert_sorted: Sentence-BERT-only ranking
# - df_combined_sorted: Combined Doc2Vec+SBERT ranking

#Print the top 5 from combined

df_combined_sorted.head(5)

#Add the 'star' column: Top 20 candidates get a value of 1, others get 0
df_combined_sorted['star'] = 0
df_combined_sorted.loc[:19, 'star'] = 1  # Top 20 (0-based index implies 0-19)

#Now the dataframe df_combined_sorted includes a 'star' column

df_combined_sorted

#Define features and target
X = df_combined_sorted[['combined_similarity', 'scaled_connection']].values
y = df_combined_sorted['ranking'].values

#Split the data into train & test sets split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Initiate the LTR model
ltr_model = xgb.XGBRanker(
    objective='rank:pairwise',  # Use appropriate LTR objective
    learning_rate=0.1,
    max_depth=6,
    n_estimators=100,
    random_state=42
)

#Fit the LTR model
ltr_model.fit(X_train, y_train, group=[len(X_train)])

#Evaluate the LTR model (ranking score predictions)
predictions = ltr_model.predict(X_test)
#Predicted ranking scores
print(predictions)



